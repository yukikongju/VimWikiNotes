# IFT3700 - TP 1: Biais Cognitifs (p-value et Correction de Benferonni; Régression vers la moyenne); Naive Bayes et Laplace Smoothing

## Part 1 - Biais Cognitifs

### Biais Cognitif #1: p-value et correction de bonferroni

En stats, on se sert du p-value pour déterminer s'il existe une différence 
entre les données observées. Par contre, on peut se tromper si on calcule 
le p-value à plusieurs reprises. (ex: checker si mcdo, burger king, ... ont des 
effets)

Pour éviter cette erreur, on veut utiliser la *correction de bonferroni*, 
qui nous dit de diviser par le nombre d'hypothèses qu'on a. 

- Without Bonferroni: P(au moins un résultat significatif) = 1 - P(aucun résultat significatif) = 1 - (0.95)^n
- With Bonferroni: P(au moins un résultat significatif) = 1 - P(aucun résultat significatif) = 1 - (1 - alpha/n)^n

### Biais Cognitif #2: Régression vers la moyenne

Lorsqu'on observe des résultats qui diffèrent de la moyenne (très bon/mauvais) 
pour un individuel, observé d'autres résultats de cette même personne ramène 
sa performance vers celle de la moyenne, car pour les autres performances 
sont plus likely d'être près de la moyenne

Ex: Pilotes d'avions. 
    * On pensait que de complimenter les pilotes d'avions pour des bonnes 
      performances avait un effet néfaste et que de scold les pilotes pour 
      une mauvaise performance améliorait les performances, mais ce n'était 
      qu'une régression vers la moyenne

## Part 2 - Naive Bayes et Laplace Smoothing

Le premier modèle qu'on a vu est le Naive Bayes, qui se base sur la formule de
Bayes.

### Comment ça marche:

* Phase Training: on veut développer une image canonique pour chaque classe (ie 
    à quoi la classe ressemble en moyenne). Pour se faire, on doit faire 
    une hypothèse importante: pixels sont indépendants (c'est là que nos erreurs 
    se produisent). Pour chaque pixel sur l'image, on veut appliquer le 
    théorème de Bayes:
	- Calculer P(x|c) -> probabilité d'avoir un vecteur appartenant à la classe 
	  au pixel donné
	- Calculer P(c): prior -> proba d'avoir une image de classe c dans le jeu de donnée
	- Calculer P(x): marginal -> proba d'avoir le pixel __ dans le jeu de donnée
	  en général

### Hypothèse de l'indépendance


### Pourquoi a-t-on besoin de Laplace Smoothing

Pour calculer P(x), on a besoin de conditionner sur la somme des P(x_i|c). Or, 
si un des pixels est toujours à zéro, alors P(x_i|c) est 0 et le produit est 
zéro. Pour ce faire, on utilise Laplace smoothing pour ajuster:

P_ij = # (pixels i de valeur 1 pour les données de la classe j) +1 /
	nb total de données de la classe j +2




Étapes:
1. Reformat Data: 
   - convert into good data type
   - normalize and categorize if necessary
2. 


